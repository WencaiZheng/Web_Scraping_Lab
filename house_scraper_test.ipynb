{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# fake a header of a browser\n",
    "headers = {'User-Agent': 'Chrome/39.0.2171.95'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earning data from yahoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"UBER\"\n",
    "\n",
    "the_url = \"https://finance.yahoo.com/calendar/earnings?symbol={0}\".format(ticker)\n",
    "# fake a header of a browser\n",
    "response = requests.get(the_url, headers=headers)\n",
    "# beatifulsoup find all the right tags\n",
    "soup = BeautifulSoup(response.text, features='lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eraning_time=soup.find_all(attrs={\"aria-label\": \"Earnings Date\"})\n",
    "est_eps=soup.find_all(attrs={\"aria-label\": \"EPS Estimate\"})\n",
    "rpt_eps=soup.find_all(attrs={\"aria-label\": \"Reported EPS\"})\n",
    "surprise =soup.find_all(attrs={\"aria-label\": \"Surprise(%)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "earning_release_time = list(map(lambda x:pd.to_datetime(x.text[:-3]),eraning_time))\n",
    "estimated_eps = list(map(lambda x:x.text,est_eps))\n",
    "reported_eps = list(map(lambda x:x.text,rpt_eps))\n",
    "surprise_percentage = list(map(lambda x:x.text,surprise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "earning_release = pd.DataFrame([earning_release_time,estimated_eps,reported_eps,surprise_percentage]).T\n",
    "earning_release.columns = [\"EarningsDate\",\"EstimateEPS\",\"ReportedEPS\",\"Surprise\"]\n",
    "earning_release.set_index(\"EarningsDate\",drop=True,inplace=True)\n",
    "earning_release.replace(\"N/A\",np.nan,inplace=True)\n",
    "earning_release = earning_release.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap the earning news from seeking alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_url = \"https://seekingalpha.com/symbol/{0}\".format(ticker)\n",
    "# fake a header of a browser\n",
    "headers = {'User-Agent': 'Chrome/39.0.2171.95'}\n",
    "response = requests.get(the_url, headers=headers)\n",
    "# beatifulsoup find all the right tags\n",
    "soup = BeautifulSoup(response.text, features='lxml')\n",
    "head_url =\"https://seekingalpha.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['/news/3571490-uber-eps-misses-0_80-beats-on-revenue']"
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "key_url=[]\n",
    "key_words = \"eps\"\n",
    "inews=soup.find_all(attrs={\"sasource\": re.compile(\"qp_latest\")})\n",
    "[key_url.append(i.get('href')) for i in inews if key_words in i.get('href')]\n",
    "if len(key_url) != 0:\n",
    "    iurl = key_url[0]\n",
    "    response = requests.get(head_url+, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, features='lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'/news/3571490-uber-eps-misses-0_80-beats-on-revenue'"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "inews=soup.find_all(href=re.compile(\"eps\"))\n",
    "inews[-1].get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inews=soup.find_all(\"div\",class_=\"symbol_article\",)\n",
    "for i in inews:\n",
    "    i.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the headlines news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_url = \"https://seekingalpha.com/market-news\"\n",
    "# fake a header of a browser\n",
    "headers = {'User-Agent': 'Chrome/39.0.2171.95'}\n",
    "response = requests.get(the_url, headers=headers)\n",
    "# beatifulsoup find all the right tags\n",
    "soup = BeautifulSoup(response.text, features='lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_news_sa=soup.find_all(attrs={\"sasource\": re.compile(\"market_news_all\")})\n",
    "comment_news_sa=soup.find_all(attrs={\"sasource\": re.compile(\"market_news_commentary\")})\n",
    "inews=soup.find_all(attrs={\"sasource\": re.compile(\"market_news_h\")})\n",
    "for i in inews:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['/news/3575345-icahn-buys-delek-boost-stake-in-occidental',\n '/news/3575343-appaloosa-management-buys-twitter-exits-caesars-cuts-facebook']"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "key_word = \"buy\" #[\"buy\",\"exit\",\"cut\"]\n",
    "the_url = \"https://seekingalpha.com/market-news\"\n",
    "# fake a header of a browser\n",
    "response = requests.get(the_url, headers=headers)\n",
    "# beatifulsoup find all the right tags\n",
    "soup = BeautifulSoup(response.text, features='lxml')\n",
    "inews = soup.find_all(href=re.compile(key_word))\n",
    "# if key word is in the website address\n",
    "key_url = [i.get('href') for i in inews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earnings incoming companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_day = 3\n",
    "today_date = pd.to_datetime(datetime.date.today())\n",
    "target_date = pd.to_datetime(datetime.date.today()+datetime.timedelta(days = recent_day))\n",
    "target_date\n",
    "\n",
    "all_df =  pd.DataFrame()\n",
    "for i in range(1,30):\n",
    "    the_url = f'https://seekingalpha.com/earnings/earnings-calendar/{i}'\n",
    "    response = requests.get(the_url, headers=headers)\n",
    "    # beatifulsoup find all the right tags\n",
    "    soup = BeautifulSoup(response.text, features='lxml')\n",
    "    tic = list(map(lambda x:x.text,soup.find_all('a',class_='sym')))\n",
    "    date = list(map(lambda x:x.text,soup.find_all('span',class_='release-date')))\n",
    "    time = list(map(lambda x:x.text,soup.find_all('span',class_='release-time')))\n",
    "    name = list(map(lambda x:x.text,soup.find_all('span',class_='ticker-name')))\n",
    "    df = pd.DataFrame([tic,name,date,time]).T\n",
    "    all_df = pd.concat([all_df,df],axis=0)\n",
    "    if pd.to_datetime(all_df.iloc[-1,2])>target_date:break\n",
    "\n",
    "\n",
    "all_df.columns=[\"Ticker\",\"Name\",\"Date\",\"Time\"]\n",
    "all_df.index = all_df.Ticker\n",
    "all_df.Date = pd.to_datetime(all_df.Date)\n",
    "all_df = all_df[all_df.Date <=  target_date].sort_values(by=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_names = pd.read_csv(\"dictionary\\\\SP500.csv\",index_col=0)\n",
    "all_df_s = all_df.join(sp_names,how =\"inner\").iloc[:,[1,2,3,7]]\n",
    "\n",
    "rs1000_names = pd.read_csv(\"dictionary\\\\RU1000.csv\",index_col=0)\n",
    "all_df_r1 = all_df.join(rs1000_names,how =\"inner\").iloc[:,:]\n",
    "\n",
    "cn_names = pd.read_csv(\"dictionary\\\\CN.csv\",index_col=0)\n",
    "all_df_c = all_df.join(cn_names,how =\"inner\").iloc[:,[1,2,3,8]].sort_values(by=\"Date\")\n",
    "\n",
    "rs3000_names = pd.read_csv(\"dictionary\\\\RU3000.csv\",index_col=0)\n",
    "all_df_r3 = all_df.join(rs3000_names,how =\"inner\").iloc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'example\\\\Public_earnings_dates_May2020.csv' does not exist: b'example\\\\Public_earnings_dates_May2020.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5f735d9bc34c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"example\\\\Public_earnings_dates_May2020.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTicker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\APP\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\APP\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\APP\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\APP\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\APP\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'example\\\\Public_earnings_dates_May2020.csv' does not exist: b'example\\\\Public_earnings_dates_May2020.csv'"
     ]
    }
   ],
   "source": [
    "exp = pd.read_csv(\"examples\\\\Public_earnings_dates_May2020.csv\")\n",
    "exp.index = list(map(lambda x:x.split(\" \")[0],exp.Ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_all = all_df_r3.join(exp,how=\"outer\",lsuffix='_left', rsuffix='_right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_all.fillna(0,inplace = True)\n",
    "\n",
    "temp_all[temp_all.Ticker_left ==0].index.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['ADVM', 'AMEH', 'AVRO', 'CVGI', 'HALL', 'HBB', 'HFFG', 'HUD',\n       'LGF.A', 'M', 'NWFL', 'OLP', 'PGR', 'RCL', 'RDI', 'RJF', 'ROSE',\n       'RRTS', 'RTW', 'SENEA', 'SIEB', 'SPN', 'SRT', 'USAT', 'WSM'],\n      dtype=object)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "temp_all[temp_all.Ticker_right ==0].index.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earning names from YAHOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_day = 5\n",
    "today_date = str(pd.to_datetime(datetime.date.today()).date())\n",
    "target_date = str(pd.to_datetime(datetime.date.today()+datetime.timedelta(days = recent_day)).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_url = f'https://finance.yahoo.com/calendar/earnings?&day={target_date}'\n",
    "response = requests.get(the_url, headers=headers)\n",
    "# beatifulsoup find all the right tags\n",
    "soup = BeautifulSoup(response.text, features='lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic =soup.find_all(attrs={\"aria-label\": \"Symbol\"})\n",
    "name =soup.find_all(attrs={\"aria-label\": \"Company\"})\n",
    "earning_call_time=soup.find_all(attrs={\"aria-label\": \"Earnings Call Time\"})\n",
    "eps_est =soup.find_all(attrs={\"aria-label\": \"Surprise(%)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[]"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "tic =soup.find_all(\"td\")\n",
    "tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_url = f'https://finance.yahoo.com/calendar/earnings?&day={target_date}'\n",
    "\n",
    "response = requests.get(the_url, headers=headers)\n",
    "\n",
    "page = response.text.replace(\"<!-->\", \"\")\n",
    "soup = BeautifulSoup(response.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bitbasecondad6d302f723a043ac8e65e786f084e6f6",
   "display_name": "Python 3.6.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}